{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c4da4c0-2bc5-4e9f-866e-634350060b19",
   "metadata": {},
   "source": [
    "<h1>Classification Tasks on Audio</h1>\n",
    "\n",
    "<h2>PyTorch</h2>\n",
    "<h3>Prediction Waveforms</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2975258e-ea18-4314-9adc-8312489c8449",
   "metadata": {},
   "source": [
    "<p>We will remake the above tasks, more-or-less, with PyTorch.</p>\n",
    "\n",
    "<p>These two libraries... could not be more different.</p>\n",
    "\n",
    "<p>Let's start by loading our dataset. Then, we are going to build a dataloader class that will do the data processing for us but to keep us honest (and reproducible) we will shuffle that mamma jamma with a fixed random state as before.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d884d204-d15d-4e67-b9d6-6beb21ae1f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "RANDOM_STATE = 1\n",
    "\n",
    "with open('osc_dataset.pickle', 'rb') as handle:\n",
    "    df = pickle.load(handle)\n",
    "\n",
    "df = df.sample(frac=1, random_state=RANDOM_STATE)\n",
    "\n",
    "df['osc_cat'], osc_uniques = pd.factorize(df['osc'])\n",
    "df['osc_cat'] = np.asarray(df['osc_cat']).astype('float32')\n",
    "df['freq_cat'], freq_uniques = pd.factorize(df['freq'])\n",
    "df['freq_cat'] = np.asarray(df['freq_cat']).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d9ba6e-84d2-4343-9ba2-b40a29820617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MakeDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, col_name, set, split):\n",
    "        self.split = split\n",
    "        self.num_samps = df.shape[0]\n",
    "        self.get_splits()\n",
    "        self.feature = df['audio'].iloc[self.sample_splits[set][0]: self.sample_splits[set][1]]\n",
    "        self.target = df[col_name].iloc[self.sample_splits[set][0]: self.sample_splits[set][1]]\n",
    "\n",
    "    def get_splits(self):\n",
    "        #This is all weirdly confusing\n",
    "        #You can use sklearn's Train-Test split function\n",
    "        #but instead we will hammmer the wall a bit\n",
    "        self.sample_splits = {}\n",
    "        index_cum = 0\n",
    "        for set in ['train', 'val', 'test']:\n",
    "            index = index_cum + int(self.split[set] * self.num_samps)\n",
    "            self.sample_splits[set] = (index_cum, index)\n",
    "            index_cum = index\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.feature.iloc[index]\n",
    "        y = self.target.iloc[index]\n",
    "        return X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbe148b-04dd-43cc-a382-a7a1e2092f29",
   "metadata": {},
   "source": [
    "<p>Up next, let's build our model class by subclassing nn.Module.</p>\n",
    "\n",
    "<p>In Tensorflow, Conv1D layers by default expect the input to be $(batch\\_size, \\; length, \\; num\\_filters)$. In PyTorch, the Conv1D layers expect the input to be $(batch\\_size, \\; num\\_filters, \\; length)$</p>\n",
    "\n",
    "<p>There are a collection of <em>lazy</em> PyTorch layers that infer their input and output sizes after running for the first time that would have prevented some aggressive noodle scratching when having to algorithmically build out the network. Instead, the below class breaks out some good ol' fashion dimesionality calculations.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54085dee-049c-4852-bfc7-83aa7a9ab867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class OscClass(nn.Module):\n",
    "\n",
    "    def __init__(self, output_size, \n",
    "                 tensor_shape, num_conv, \n",
    "                 base_filter, base_kernel, \n",
    "                 stride, pool, activation):\n",
    "        super().__init__()\n",
    "        self.num_conv = num_conv\n",
    "        in_filter = 1\n",
    "        L_out = tensor_shape\n",
    "        for i in range(self.num_conv):\n",
    "            #This is confusing but setattr lets us define class attributes\n",
    "            #with strings... Helpful for when one might want to\n",
    "            #programmatically define the number of Conv Layers\n",
    "            setattr(self, 'conv' + str(i), nn.Conv1d(in_filter, (out_filter := base_filter * (2 ** i)), base_kernel, stride=stride))\n",
    "            in_filter = out_filter\n",
    "            L_out = self.calculate_conv_size(L_out, base_kernel, stride)\n",
    "            L_out = self.calculate_pool_size(L_out, pool)\n",
    "\n",
    "        self.pool = nn.MaxPool1d(pool)\n",
    "        #Same-ish as above, use getattr to use\n",
    "        #a string name to call the \"activation\" module \n",
    "        #from nn\n",
    "        self.activation = getattr(nn, activation)()\n",
    "        #We could use lazy linear and not worry about calculating the \n",
    "        #number of input nodes but we might as well have a solid grasp of the\n",
    "        #network mechanics\n",
    "        self.output_layer = nn.Linear(L_out * out_filter, output_size)\n",
    "        \n",
    "    def calculate_conv_size(self, L_in, ker, strid, dil=1, pad=0):\n",
    "        return int(np.floor((L_in + 2 * pad - dil * (ker - 1) - 1) / strid + 1))\n",
    "\n",
    "    def calculate_pool_size(self, L_in, ker, strid=None, dil=1, pad=0):\n",
    "        if not strid:\n",
    "            strid = ker\n",
    "        return int(np.floor((L_in + 2 * pad - dil * (ker - 1) - 1) / strid + 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Reshape the tensor to be (batch_size, 1, samp_rate)\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        for i in range(self.num_conv):\n",
    "            #I know this is confusing as shit but the getattr function\n",
    "            #gives me the ability to programmatically call class\n",
    "            #attributes/methods with string names!\n",
    "            x = self.pool(self.activation(getattr(self, 'conv' + str(i))(x)))\n",
    "        #Leave out batch dim in the flatten\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d666b32-2a58-4b85-90bb-6faa6a2999c7",
   "metadata": {},
   "source": [
    "<p>Like in our Tensorflow model, the following will create some training classes.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1476db3-5201-4f19-a75e-5eba01acdd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_classes(max_epochs, learning_rate, model):\n",
    "    CEL_loss = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=np.exp(-1))\n",
    "    return optimizer, scheduler, CEL_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eee40a-623b-4ac5-9dc7-8ed38c330fd4",
   "metadata": {},
   "source": [
    "<p>Finally, let's make a class to handle the training, validation, and test loops!</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2d4b3d-ca50-422a-b110-2f4b00edd684",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainLoop():\n",
    "    def __init__(self, datasets, model, max_epochs,\n",
    "                optimizer, scheduler, loss_fun):\n",
    "        self.datasets = datasets\n",
    "        self.model = model\n",
    "        self.max_epochs = max_epochs\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.loss_fun = loss_fun\n",
    "\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        self.device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "        if use_cuda:\n",
    "            self.model = self.model.to(self.device)\n",
    "\n",
    "        self.history = {'train':{'loss': [],\n",
    "                           'acc': []},\n",
    "                  'val':{'loss': [],\n",
    "                         'acc': []},\n",
    "                  'test':{'loss': [],\n",
    "                         'acc': []},\n",
    "                        'lr': []\n",
    "                  }\n",
    "\n",
    "    def data_loop(self, set):\n",
    "        if set == 'train':\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        loss_temp = []\n",
    "        for X, y in self.datasets[set]:\n",
    "            #Cast tensor to longtensor! and send to GPU\n",
    "            X, y = X.to(self.device), y.type(torch.LongTensor).to(self.device)\n",
    "            pred = self.model.forward(X)\n",
    "            loss = self.loss_fun(pred, y)\n",
    "            if set == 'train':\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            loss_temp.append(loss.item())\n",
    "            y_pred.extend(torch.argmax(pred.data, -1).tolist())\n",
    "            y_true.extend(y.tolist())\n",
    "        self.history[set]['loss'].append(np.mean(loss_temp))\n",
    "        self.history[set]['acc'].append(accuracy_score(y_pred, y_true))\n",
    "\n",
    "    def collect_lr(self):\n",
    "        return self.history['lr'].append(self.optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "\n",
    "\n",
    "    def full_loop(self):\n",
    "        for epoch in range(self.max_epochs):\n",
    "            self.collect_lr()\n",
    "            \n",
    "            self.data_loop('train')\n",
    "\n",
    "            self.data_loop('val')\n",
    "\n",
    "            if epoch > int(self.max_epochs / 2):\n",
    "                self.scheduler.step()\n",
    "            \n",
    "        self.data_loop('test')\n",
    "\n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4433fa9d-b4a7-403c-a082-05eeccc993e9",
   "metadata": {},
   "source": [
    "<p>We will remake the hyperparameter search conditions but mostly because the activations have different names in Tensorflow vs. PyTorch.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab69165-1615-4ecb-a90d-e14ebc492ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "\n",
    "NUM_EXP = 100\n",
    "\n",
    "batch_size = [32, 64, 128, 256]\n",
    "learning_rate = [0.001, 0.0005, 0.002]\n",
    "max_epochs = [3, 5, 10, 15]\n",
    "base_filters = [2, 4, 8]\n",
    "base_kernels = [4, 8, 10]\n",
    "base_stride = [1, 2, 4]\n",
    "base_pool = [3, 4, 5]\n",
    "num_conv_blocks = [3, 4, 5]\n",
    "activations = ['ReLU', 'SELU', 'ELU']\n",
    "\n",
    "hyperparameters = list(itertools.product(batch_size, learning_rate, \n",
    "                                         max_epochs, base_filters,\n",
    "                                         base_kernels, base_stride,\n",
    "                                         base_pool, num_conv_blocks,\n",
    "                                         activations))\n",
    "\n",
    "parameters = []\n",
    "\n",
    "for i in np.random.choice(np.arange(len(hyperparameters)), size=NUM_EXP, replace=False):\n",
    "    temp = {}\n",
    "    temp['batch_size'] = hyperparameters[i][0]\n",
    "    temp['init_learning_rate'] = hyperparameters[i][1]\n",
    "    temp['max_epochs'] = hyperparameters[i][2]\n",
    "    temp['base_filters'] = hyperparameters[i][3]\n",
    "    temp['base_kernels'] = hyperparameters[i][4]\n",
    "    temp['base_stride'] = hyperparameters[i][5]\n",
    "    temp['base_pool'] = hyperparameters[i][6]\n",
    "    temp['num_conv_blocks'] = hyperparameters[i][7]\n",
    "    temp['activations'] = hyperparameters[i][8]\n",
    "    parameters.append(temp)\n",
    "\n",
    "parameters = sorted(parameters, key=lambda x: x['batch_size'])\n",
    "\n",
    "del hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4008ea9-ba7d-40e6-b97d-1b89fbcedba0",
   "metadata": {},
   "source": [
    "<p>Finally, let's do our stochastic hyperparameter search pattern!</p>\n",
    "\n",
    "<p>Notice that we are using DataLoader!</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7b4c0e-3371-4d19-b4c9-aeb41eaebca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e179dfb1-be31-4410-9743-3a7320db30c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "search_history = []\n",
    "prev_batch_size = -1\n",
    "data_col = 'osc_cat'\n",
    "tensor_shape = df.audio.iloc[0].shape[0]\n",
    "output_size = pd.unique(df[data_col]).shape[0]\n",
    "\n",
    "if train:\n",
    "    for i, params in enumerate(parameters):\n",
    "        torch.cuda.empty_cache()\n",
    "        print('{}'.format(i), '='*10)\n",
    "        try:\n",
    "            temp = {}\n",
    "            if params['batch_size'] != prev_batch_size:\n",
    "                dataset_params = {'batch_size': params['batch_size']}\n",
    "                datasets = {'train': data.DataLoader(MakeDataset(df, data_col, 'train', SPLIT_DICT), **dataset_params),\n",
    "                            'val': data.DataLoader(MakeDataset(df, data_col, 'val', SPLIT_DICT), **dataset_params),\n",
    "                            'test': data.DataLoader(MakeDataset(df, data_col, 'test', SPLIT_DICT), **dataset_params)}\n",
    "                prev_batch_size = params['batch_size']\n",
    "\n",
    "            start = time.time()\n",
    "            \n",
    "            model = OscClass(output_size, tensor_shape, \n",
    "                             params['num_conv_blocks'], params['base_filters'], \n",
    "                             params['base_kernels'], params['base_stride'], \n",
    "                             params['base_pool'], params['activations']) \n",
    "            optimizer, scheduler, CEL_loss = build_train_classes(params['max_epochs'], params['init_learning_rate'], model)\n",
    "            train_loop = TrainLoop(datasets, model, params['max_epochs'], \n",
    "                       optimizer, scheduler, CEL_loss)\n",
    "\n",
    "            history = train_loop.full_loop()\n",
    "\n",
    "            temp['train_acc'] = history['train']['acc']\n",
    "            temp['train_loss'] = history['train']['loss']\n",
    "            temp['val_acc'] = history['val']['acc']\n",
    "            temp['val_loss'] = history['val']['loss']\n",
    "            temp['test_acc'] = history['test']['acc']\n",
    "            temp['test_loss'] = history['test']['loss']\n",
    "            temp['learning_rates'] = history['lr']\n",
    "            temp['realized_epochs'] = len(history['lr'])\n",
    "            temp['num_params'] = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "            print('<{} Filters> <{} Kernel> <{} Stride> <{} Pool> <{} Conv Blocks>'.format(params['base_filters'], params['base_kernels'], \n",
    "                                                                                           params['base_stride'], params['base_pool'], \n",
    "                                                                                           params['num_conv_blocks']))\n",
    "            \n",
    "            print('<{} Epochs> <{} Batch Size> <{} Initial Learning Rate>'.format(params['max_epochs'], params['batch_size'], params['init_learning_rate']))\n",
    "            temp['train_time'] = time.time() - start\n",
    "            print('Accuracy = {} Parameters = {}'.format(temp['test_acc'], temp['num_params']))\n",
    "            print('Training took {0:.2f} seconds'.format(temp['train_time']))\n",
    "\n",
    "            temp.update(params)\n",
    "            search_history.append(temp)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            temp['error'] = e\n",
    "            temp['accuracy'] = -1\n",
    "            print('Not trainable')\n",
    "\n",
    "train = False\n",
    "\n",
    "training_df_osc = pd.DataFrame(search_history)\n",
    "\n",
    "with open('training_df_oscPT.xlsx', 'wb') as handle:\n",
    "    pickle.dump(training_df_osc, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdc8e15-e429-4530-93a9-cc303be25e6f",
   "metadata": {},
   "source": [
    "<h3>Predicting Pitch</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bd2d5d-2987-4ae1-8dff-157c7c8f049d",
   "metadata": {},
   "source": [
    "<p>Then, we will run the search again for the pitch classification task with the above PyTorch loop.</p>\n",
    "\n",
    "<p>The only change we have to do it change the target data column of the input data frame in the MakeDataset class.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9068eb3-d3d6-48b9-a0a5-9e401bc7bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f776be4c-5373-4f68-a120-6674ed3f03d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_history = []\n",
    "prev_batch_size = -1\n",
    "data_col = 'freq_cat'\n",
    "tensor_shape = df.audio.iloc[0].shape[0]\n",
    "output_size = pd.unique(df[data_col]).shape[0]\n",
    "\n",
    "if train:\n",
    "    for i, params in enumerate(parameters):\n",
    "        torch.cuda.empty_cache()\n",
    "        print('{}'.format(i), '='*10)\n",
    "        try:\n",
    "            temp = {}\n",
    "            if params['batch_size'] != prev_batch_size:\n",
    "                dataset_params = {'batch_size': params['batch_size']}\n",
    "                datasets = {'train': data.DataLoader(MakeDataset(df, data_col, 'train'), **dataset_params),\n",
    "                            'val': data.DataLoader(MakeDataset(df, data_col, 'val'), **dataset_params),\n",
    "                            'test': data.DataLoader(MakeDataset(df, data_col, 'test'), **dataset_params)}\n",
    "                prev_batch_size = params['batch_size']\n",
    "\n",
    "            start = time.time()\n",
    "            \n",
    "            model = OscClass(output_size, tensor_shape, \n",
    "                             params['num_conv_blocks'], params['base_filters'], \n",
    "                             params['base_kernels'], params['base_stride'], \n",
    "                             params['base_pool'], params['activations']) \n",
    "            optimizer, scheduler, CEL_loss = build_train_classes(params['max_epochs'], params['init_learning_rate'], model)\n",
    "            train_loop = TrainLoop(datasets, model, params['max_epochs'], \n",
    "                       optimizer, scheduler, CEL_loss)\n",
    "\n",
    "            history = train_loop.full_loop()\n",
    "\n",
    "            temp['train_acc'] = history['train']['acc']\n",
    "            temp['train_loss'] = history['train']['loss']\n",
    "            temp['val_acc'] = history['val']['acc']\n",
    "            temp['val_loss'] = history['val']['loss']\n",
    "            temp['test_acc'] = history['test']['acc']\n",
    "            temp['test_loss'] = history['test']['loss']\n",
    "            temp['learning_rates'] = history['lr']\n",
    "            temp['realized_epochs'] = len(history['lr'])\n",
    "            temp['num_params'] = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "            print('<{} Filters> <{} Kernel> <{} Stride> <{} Pool> <{} Conv Blocks>'.format(params['base_filters'], params['base_kernels'], \n",
    "                                                                                           params['base_stride'], params['base_pool'], \n",
    "                                                                                           params['num_conv_blocks']))\n",
    "            \n",
    "            print('<{} Epochs> <{} Batch Size> <{} Initial Learning Rate>'.format(params['max_epochs'], params['batch_size'], params['init_learning_rate']))\n",
    "            temp['train_time'] = time.time() - start\n",
    "            print('Accuracy = {} Parameters = {}'.format(temp['test_acc'], temp['num_params']))\n",
    "            print('Training took {0:.2f} seconds'.format(temp['train_time']))\n",
    "\n",
    "            temp.update(params)\n",
    "            search_history.append(temp)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            temp['error'] = e\n",
    "            temp['accuracy'] = -1\n",
    "            print('Not trainable')\n",
    "\n",
    "train = False\n",
    "\n",
    "training_df_freq = pd.DataFrame(search_history)\n",
    "\n",
    "with open('training_df_freqPT.xlsx', 'wb') as handle:\n",
    "    pickle.dump(training_df_freq, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591fe01b-06bc-419a-bbfc-b68edb9d3261",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
